{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5613279067707206342\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "\n",
    "print(local_device_protos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dense, Reshape, Concatenate, Conv2D, LeakyReLU, Flatten, Dropout, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(in_shape=(224, 224, 1), n_classes=144):  # providing class info\n",
    "    # label input\n",
    "    in_label = Input(shape=(1,))  # label input\n",
    "    # embedding for categorical input (each label represented by a vector of size 50)\n",
    "    # takes the class label and maps it to a random vector of size 50\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    n_nodes = in_shape[0] * in_shape[1]  # 224x224=50176\n",
    "    # labels have to be the same size as the image\n",
    "    # 50176x1\n",
    "    li = Dense(n_nodes)(li)\n",
    "    li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "\n",
    "    # Image input\n",
    "    in_image = Input(shape=in_shape)  # image input 32x32x3\n",
    "\n",
    "    # Concat label and image\n",
    "    # 4 channels, 3 for image and the other for labels\n",
    "    merge = Concatenate()([in_image, li])  # 32x32x4\n",
    "\n",
    "    #  this part is the same an uncoditional GAN up to the output layer\n",
    "    fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(merge)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    fe = Flatten()(fe)\n",
    "    fe = Dropout(0.4)(fe)\n",
    "\n",
    "    out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "    # combine input label with input img\n",
    "    model = Model([in_image, in_label], out_layer, name='cgan_discriminator')\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = define_discriminator()\n",
    "\n",
    "# test.summary()\n",
    "\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "# plot_model(test, to_file='cgan_discriminator2.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim, n_classes=144):\n",
    "    # Label input\n",
    "    in_label = Input(shape=(1,))\n",
    "    li = Embedding(n_classes, 50)(in_label)  # shape (1, 50)\n",
    "    n_nodes = 28 * 28  # 784 - to match the dimensions for concat later\n",
    "    li = Dense(n_nodes)(li)\n",
    "    li = Reshape((28, 28, 1))(li)  # this is out label input for the model\n",
    "\n",
    "    # Latent vector input\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    n_nodes = 128 * 28 * 28  # 100352\n",
    "    gen = Dense(n_nodes)(in_lat)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Reshape((28, 28, 128))(gen)\n",
    "\n",
    "    merge = Concatenate()([gen, li])  # merge image gen and label inputs\n",
    "    gen = Conv2DTranspose(128, (4, 4), strides=(\n",
    "        2, 2), padding='same')(merge)  # upsample to 56x56\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Conv2DTranspose(128, (4, 4), strides=(\n",
    "        2, 2), padding='same')(gen)  # upsample to 112x112\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Conv2DTranspose(128, (4, 4), strides=(\n",
    "        2, 2), padding='same')(gen)  # upsample to 224x224\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "\n",
    "    out_layer = Conv2D(1, (7, 7), activation='tanh', padding='same')(\n",
    "        gen)  # output layer 224x224x1\n",
    "    model = Model([in_lat, in_label], out_layer, name='cgan_generator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22668\\1604269804.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# test = define_generator(100)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test = define_generator(100)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples(batch_size=64):\n",
    "    datagen = ImageDataGenerator()\n",
    "    sdir_gray = r'./data_gray/'\n",
    "    train_dir = os.path.join(sdir_gray, 'train_gray')\n",
    "    train_it = datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(224, 224),  # adjust this to the size of your images\n",
    "        batch_size=batch_size,  # number of images to load at a time\n",
    "        class_mode='categorical',  # use 'categorical' for multi-class problems\n",
    "        color_mode='grayscale'  # or 'rgb' TODO: make an if\n",
    "    )\n",
    "\n",
    "    # store all images and labels in a list\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(train_it)):\n",
    "        print(1)\n",
    "        images, labels = train_it.next()\n",
    "        X.append(images)\n",
    "        # extract label numbers\n",
    "        label_numbers = np.argmax(labels, axis=1)\n",
    "        y.append(label_numbers)\n",
    "\n",
    "    y = [np.array(batch_labels) for batch_labels in y]\n",
    "    y = np.concatenate(y)\n",
    "    y = y.reshape(-1, 1)  # reshape y to have shape (11520, 1)\n",
    "\n",
    "    X = np.concatenate(X)\n",
    "    X = X.astype('float32')\n",
    "    X = (X - 127.5) / 127.5  # normalize to [-1, 1]\n",
    "\n",
    "    return [X, y]\n",
    "\n",
    "\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix = np.random.randint(0, images.shape[0], n_samples)\n",
    "    # select images and labels\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    y = np.ones((n_samples, 1))  # generate 'real' class labels (1)\n",
    "\n",
    "    return [X, labels], y  # [image, label], real\n",
    "\n",
    "\n",
    "# latent points = latent vector + label\n",
    "def generate_latent_points(latent_dim, n_samples, n_classes=144):\n",
    "    # generate points in the latent space\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)  # TODO: print this shape\n",
    "    # generate labels\n",
    "    labels = np.random.randint(0, n_classes, n_samples)\n",
    "\n",
    "    return [z_input, labels]  # random vector + random label\n",
    "\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    images = generator.predict([z_input, labels_input])\n",
    "    # create 'fake' class labels (0)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "\n",
    "    return [images, labels_input], y  # [image, label], fake\n",
    "\n",
    "\n",
    "def print_dataset_in_file(dataset):\n",
    "    print(dataset[0].shape)  # (11520, 224, 224, 1)\n",
    "    print(len(dataset[0]))\n",
    "    print(dataset[1].shape)  # (11520, 1)\n",
    "    print(len(dataset[1]))\n",
    "\n",
    "    with open('output.txt', 'w') as f:\n",
    "        for i in range(len(dataset[1])):\n",
    "            print(dataset[0][i], file=f)\n",
    "            print(dataset[1][i], file=f)\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "    d_model.trainable = False  # discriminator is trained separately, so make it non-trainable\n",
    "    \n",
    "    ## connect generator and discriminator\n",
    "    # first get noise and label inputs from generator\n",
    "    gen_noise, gen_label = g_model.input # latent vector size and label size\n",
    "    # get image output from generator\n",
    "    gen_output = g_model.output\n",
    "    \n",
    "    # generate iamge output corresponding to the label\n",
    "    gan_output = d_model([gen_output, gen_label])\n",
    "    # define GAN model as taking noise and label and outputting a classification\n",
    "    model = Model([gen_noise, gen_label], gan_output)\n",
    "    \n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=64):\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "\n",
    "    for i in range(n_epochs):  # enumerate epochs\n",
    "        for j in range(bat_per_epo):  # enumerate batches\n",
    "            [X_real, labels_real], y_real = generate_real_samples(\n",
    "                dataset, half_batch)\n",
    "\n",
    "            d_loss_real, _ = d_model.train_on_batch(\n",
    "                [X_real, labels_real], y_real)  # train D on real images\n",
    "\n",
    "            [X_fake, labels], y_fake = generate_fake_samples(\n",
    "                g_model, latent_dim, half_batch)\n",
    "\n",
    "            d_loss_fake, _ = d_model.train_on_batch(\n",
    "                [X_fake, labels], y_fake)  # train D on fake images\n",
    "\n",
    "            [z_input, labels_input] = generate_latent_points(\n",
    "                latent_dim, n_batch)  # generator input\n",
    "            # label generated images as true to fake discriminator\n",
    "            y_gan = np.ones((n_batch, 1))\n",
    "\n",
    "            # train G on fake images with inverted labels\n",
    "            g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
    "\n",
    "            # print losses on this batch\n",
    "            print('epoch>%d, batch:%d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
    "                  (i+1, j+1, bat_per_epo, d_loss_real, d_loss_fake, g_loss))\n",
    "\n",
    "    # save the generator model\n",
    "    g_model.save('cgan_generator_clock.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimau\\Anaconda3\\envs\\clock\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11520 images belonging to 144 classes.\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'JpegImageFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22668\\3884199840.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mgan_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_gan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# load image data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_real_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m# train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22668\\3113969695.py\u001b[0m in \u001b[0;36mload_real_samples\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_it\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_it\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# extract label numbers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dimau\\Anaconda3\\envs\\clock\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;31m# The transformation of images is not under thread lock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;31m# so it can be done in parallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dimau\\Anaconda3\\envs\\clock\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    375\u001b[0m                 \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_aspect_ratio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             )\n\u001b[1;32m--> 377\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[1;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[1;31m# but not PIL images.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dimau\\Anaconda3\\envs\\clock\\lib\\site-packages\\keras\\utils\\image_utils.py\u001b[0m in \u001b[0;36mimg_to_array\u001b[1;34m(img, data_format, dtype)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;31m# or (channel, height, width)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;31m# but original PIL image has format (width, height, channel)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"channels_first\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'JpegImageFile'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "d_model = define_discriminator()\n",
    "# create the generator\n",
    "g_model = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "# train model\n",
    "# train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
